{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle泰坦尼克之灾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤:\n",
    "\n",
    "一.导入数据包与数据集\n",
    "\n",
    "二.数据分析\n",
    "\n",
    "1. 总体预览:了解每列数据的含义,数据的格式等\n",
    "2. .数据初步分析,使用统计学与绘图:初步了解数据之间的相关性,为构造特征工程以及模型建立做准备\n",
    "\n",
    "三.特征工程\n",
    "\n",
    "1. 根据业务,常识,以及第二步的数据分析构造特征工程.\n",
    "2. 将特征转换为模型可以辨别的类型(如处理缺失值,处理文本进行等)\n",
    "\n",
    "四.模型选择\n",
    "\n",
    "1. 根据目标函数确定学习类型,是无监督学习还是监督学习,是分类问题还是回归问题等.\n",
    "2. 比较各个模型的分数,然后取效果较好的模型作为基础模型.\n",
    "\n",
    "五.修改特征和模型参数\n",
    "\n",
    "1. 可以通过添加或者修改特征,提高模型的上限.\n",
    "2. 通过修改模型的参数,是模型逼近上限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.导入数据包与数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入相关数据包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './datasets'\n",
    "\n",
    "train = pd.read_csv('%s/%s' % (root_path, 'train.csv'))\n",
    "test = pd.read_csv('%s/%s' % (root_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二.数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.总体预览\n",
    "了解每列数据的含义,数据的格式等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回每列列名,该列非nan值个数,以及该列类型\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回数值型变量的统计量\n",
    "# train.describe(percentiles=[0.00, 0.25, 0.5, 0.75, 1.00])\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2..数据初步分析,使用统计学与绘图\n",
    "目的:初步了解数据之间的相关性,为构造特征工程以及模型建立做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存活人数\n",
    "train['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)数值型数据协方差,corr()函数\n",
    "来个总览,快速了解个数据的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#相关性协方差表,corr()函数,返回结果接近0说明无相关性,大于0说明是正相关,小于0是负相关.\n",
    "train_corr = train.drop('PassengerId',axis=1).corr()\n",
    "train_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画出相关性热力图\n",
    "a = plt.subplots(figsize=(15,9))#调整画布大小\n",
    "a = sns.heatmap(train_corr, vmin=-1, vmax=1 , annot=True , square=True)#画热力图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)各个数据与结果的关系\n",
    "进一步探索分析各个数据与结果的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ①Pclass,乘客等级,1是最高级\n",
    "结果分析:可以看出Survived和Pclass在Pclass=1的时候有较强的相关性（>0.5），所以最终模型中包含该特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['Pclass'])['Pclass','Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ②Sex,性别\n",
    "结果分析:女性有更高的活下来的概率（74%）,保留该特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['Sex'])['Sex','Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ③SibSp and Parch  兄妹配偶数/父母子女数\n",
    "结果分析:这些特征与特定的值没有相关性不明显，最好是由这些独立的特征派生出一个新特征或者一组新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['SibSp','Survived']].groupby(['SibSp']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Parch','Survived']].groupby(['Parch']).mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ④Age年龄与生存情况的分析.\n",
    "结果分析:由图,可以看到年龄是影响生存情况的. \n",
    "\n",
    "但是年龄是有大部分缺失值的,缺失值需要进行处理,可以使用填充或者模型预测."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col='Survived',size=5)\n",
    "g.map(plt.hist, 'Age', bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['Age'])['Survived'].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⑤Embarked登港港口与生存情况的分析\n",
    "结果分析:C地的生存率更高,这个也应该保留为模型特征."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Embarked',hue='Survived',data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⑥其他因素\n",
    "*在数据的Name项中包含了对该乘客的称呼，如Mr、Miss等，这些信息包含了乘客的年龄、性别、也有可能包含社会地位，如Dr、Lady、Major、Master等称呼。这一项不方便用图表展示，但是在特征工程中，我们会将其提取出来,然后放到模型中。\n",
    "\n",
    "*剩余因素还有船票价格、船舱号和船票号，这三个因素都可能会影响乘客在船中的位置从而影响逃生顺序，但是因为这三个因素与生存之间看不出明显规律，所以在后期模型融合时，将这些因素交给模型来决定其重要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三.特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先将数据集合并,一起做特征工程(注意,标准化的时候需要分开处理)\n",
    "#先将test补齐,然后通过pd.apped()合并\n",
    "test['Survived'] = 0\n",
    "train_test = train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ①Pclass,乘客等级,1是最高级\n",
    "两种方式:一是该特征不做处理,可以直接保留.二是再处理:也进行分列处理(比较那种方式模型效果更好,就选那种)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test,columns=['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ②Sex,性别\n",
    "无缺失值,直接分列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test,columns=[\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③SibSp and Parch  兄妹配偶数/父母子女数\n",
    "第一次直接保留:这两个都影响生存率,且都是数值型,先直接保存.\n",
    "\n",
    "第二次进行两项求和,并进行分列处理.(兄妹配偶数和父母子女数都是认识人的数量,所以总数可能也会更好)(模型结果提高到了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这是剑豪模型后回来添加的新特征,模型的分数最终有所提高了.\n",
    "train_test['SibSp_Parch'] = train_test['SibSp'] + train_test['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test,columns = ['SibSp','Parch','SibSp_Parch']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ④Embarked \n",
    "数据有极少量(3个)缺失值,但是在分列的时候,缺失值的所有列可以均为0,所以可以考虑不填充.\n",
    "\n",
    "另外,也可以考虑用测试集众数来填充.先找出众数,再采用df.fillna()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test,columns=[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⑤ Name\n",
    "1.在数据的Name项中包含了对该乘客的称呼,将这些关键词提取出来,然后做分列处理.(参考别人的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从名字中提取出称呼： df['Name].str.extract()是提取函数,配合正则一起使用\n",
    "train_test['Name1'] = train_test['Name'].str.extract('.+,(.+)').str.extract( '^(.+?)\\.').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将姓名分类处理()\n",
    "train_test['Name1'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer' , inplace = True)\n",
    "train_test['Name1'].replace(['Jonkheer', 'Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty' , inplace = True)\n",
    "train_test['Name1'].replace(['Mme', 'Ms', 'Mrs'], 'Mrs')\n",
    "train_test['Name1'].replace(['Mlle', 'Miss'], 'Miss')\n",
    "train_test['Name1'].replace(['Mr'], 'Mr' , inplace = True)\n",
    "train_test['Name1'].replace(['Master'], 'Master' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分列处理\n",
    "train_test = pd.get_dummies(train_test,columns=['Name1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 从姓名中提取出姓做特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从姓名中提取出姓\n",
    "train_test['Name2'] = train_test['Name'].apply(lambda x: x.split('.')[1])\n",
    "\n",
    "#计算数量,然后合并数据集\n",
    "Name2_sum = train_test['Name2'].value_counts().reset_index()\n",
    "Name2_sum.columns=['Name2','Name2_sum']\n",
    "train_test = pd.merge(train_test,Name2_sum,how='left',on='Name2')\n",
    "\n",
    "#由于出现一次时该特征时无效特征,用one来代替出现一次的姓\n",
    "train_test.loc[train_test['Name2_sum'] == 1 , 'Name2_new'] = 'one'\n",
    "train_test.loc[train_test['Name2_sum'] > 1 , 'Name2_new'] = train_test['Name2']\n",
    "del train_test['Name2']\n",
    "\n",
    "#分列处理\n",
    "train_test = pd.get_dummies(train_test,columns=['Name2_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删掉姓名这个特征\n",
    "del train_test['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⑥ Fare\n",
    "该特征有缺失值,先找出缺失值的那调数据,然后用平均数填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从上面的分析,发现该特征train集无miss值,test有一个缺失值,先查看\n",
    "train_test.loc[train_test[\"Fare\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#票价与pclass和Embarked有关,所以用train分组后的平均数填充\n",
    "train.groupby(by=[\"Pclass\",\"Embarked\"]).Fare.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用pclass=3和Embarked=S的平均数14.644083来填充\n",
    "train_test[\"Fare\"].fillna(14.435422,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ⑦ Ticket\n",
    "该列和名字做类似的处理,先提取,然后分列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将Ticket提取字符列\n",
    "#str.isnumeric()  如果S中只有数字字符，则返回True，否则返回False\n",
    "train_test['Ticket_Letter'] = train_test['Ticket'].str.split().str[0]\n",
    "train_test['Ticket_Letter'] = train_test['Ticket_Letter'].apply(lambda x:np.nan if x.isnumeric() else x)\n",
    "train_test.drop('Ticket',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分列,此时nan值可以不做处理\n",
    "train_test = pd.get_dummies(train_test,columns=['Ticket_Letter'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⑧ Age\n",
    "1.该列有大量缺失值,考虑用一个回归模型进行填充.\n",
    "\n",
    "2.在模型修改的时候,考虑到年龄缺失值可能影响死亡情况,用年龄是否缺失值来构造新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这是模型就好后回来增加的新特征\n",
    "考虑年龄缺失值可能影响死亡情况,数据表明,年龄缺失的死亡率为0.19.\n",
    "\"\"\"\n",
    "train_test.loc[train_test[\"Age\"].isnull()]['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 所以用年龄是否缺失值来构造新特征\n",
    "train_test.loc[train_test[\"Age\"].isnull() ,\"age_nan\"] = 1\n",
    "train_test.loc[train_test[\"Age\"].notnull() ,\"age_nan\"] = 0\n",
    "train_test = pd.get_dummies(train_test,columns=['age_nan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用其他组特征量，采用机器学习算法来预测Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建没有['Age','Survived']的数据集\n",
    "missing_age = train_test.drop(['Survived','Cabin'],axis=1)\n",
    "#将Age完整的项作为训练集、将Age缺失的项作为测试集。\n",
    "missing_age_train = missing_age[missing_age['Age'].notnull()]\n",
    "missing_age_test = missing_age[missing_age['Age'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建训练集合预测集的X和Y值\n",
    "missing_age_X_train = missing_age_train.drop(['Age'], axis=1)\n",
    "missing_age_Y_train = missing_age_train['Age']\n",
    "missing_age_X_test = missing_age_test.drop(['Age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先将数据标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "#用测试集训练并标准化\n",
    "ss.fit(missing_age_X_train)\n",
    "missing_age_X_train = ss.transform(missing_age_X_train)\n",
    "missing_age_X_test = ss.transform(missing_age_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用贝叶斯预测年龄\n",
    "from sklearn import linear_model\n",
    "lin = linear_model.BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.fit(missing_age_X_train,missing_age_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#利用loc将预测值填入数据集\n",
    "train_test.loc[(train_test['Age'].isnull()), 'Age'] = lin.predict(missing_age_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将年龄划分是个阶段10以下,10-18,18-30,30-50,50以上\n",
    "train_test['Age'] = pd.cut(train_test['Age'], bins=[0,10,18,30,50,100],labels=[1,2,3,4,5])\n",
    "\n",
    "train_test = pd.get_dummies(train_test,columns=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⑨ Cabin\n",
    "cabin项缺失太多，只能将有无Cain首字母进行分类,缺失值为一类,作为特征值进行建模,也可以考虑直接舍去该特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cabin项缺失太多，只能将有无Cain首字母进行分类,缺失值为一类,作为特征值进行建模\n",
    "train_test['Cabin'] = train_test['Cabin'].apply(lambda x:str(x)[0] if pd.notnull(x) else x)\n",
    "train_test = pd.get_dummies(train_test,columns=['Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cabin项缺失太多，只能将有无Cain首字母进行分类,\n",
    "train_test.loc[train_test[\"Cabin\"].isnull() ,\"Cabin_nan\"] = 1\n",
    "train_test.loc[train_test[\"Cabin\"].notnull() ,\"Cabin_nan\"] = 0\n",
    "train_test = pd.get_dummies(train_test,columns=['Cabin_nan'])\n",
    "train_test.drop('Cabin',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⑩ 特征工程处理完了,划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_test[:891]\n",
    "test_data = train_test[891:]\n",
    "train_data_X = train_data.drop(['Survived'],axis=1)\n",
    "train_data_Y = train_data['Survived']\n",
    "test_data_X = test_data.drop(['Survived'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标准化\n",
    "1.线性模型需要用标准化的数据建模,而树类模型不需要标准化的数据\n",
    "\n",
    "2.处理标准化的时候,注意将测试集的数据transform到test集上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss2 = StandardScaler()\n",
    "ss2.fit(train_data_X)\n",
    "train_data_X_sd = ss2.transform(train_data_X)\n",
    "test_data_X_sd = ss2.transform(test_data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始建模\n",
    "1.可选单个模型模型有随机森林,逻辑回归,svm,xgboost,gbdt等.\n",
    "\n",
    "2.也可以将多个模型组合起来,进行模型融合,比如voting,stacking等方法\n",
    "\n",
    "3.好的特征决定模型上限,好的模型和参数可以无线逼近上限.\n",
    "\n",
    "4.我测试了多种模型,模型结果最高的随机森林,最高有0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150,min_samples_leaf=3,max_depth=6,oob_score=True)\n",
    "rf.fit(train_data_X,train_data_Y)\n",
    "\n",
    "test[\"Survived\"] = rf.predict(test_data_X)\n",
    "RF = test[['PassengerId','Survived']].set_index('PassengerId')\n",
    "RF.to_csv('RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随机森林是随机选取特征进行建模的,所以每次的结果可能都有点小差异\n",
    "#如果分数足够好,可以将该模型保存起来,下次直接调出来使用0.81339 'rf10.pkl'\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, 'rf10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param = {'C':[0.001,0.01,0.1,1,10],\"max_iter\":[100,250]}\n",
    "\n",
    "clf = GridSearchCV(lr,param,cv=5,n_jobs=-1,verbose=1,scoring=\"roc_auc\")\n",
    "\n",
    "clf.fit(train_data_X_sd,train_data_Y)\n",
    "\n",
    "#打印参数的得分情况\n",
    "clf.grid_scores_\n",
    "\n",
    "#打印最佳参数\n",
    "clf.best_params_\n",
    "\n",
    "#将最佳参数传入训练模型\n",
    "lr = LogisticRegression(clf.best_params_)\n",
    "\n",
    "lr.fit(train_data_X_sd,train_data_Y)\n",
    "\n",
    "#预测结果\n",
    "lr.predict(test_data_X_sd)\n",
    "\n",
    "#打印结果\n",
    "test[\"Survived\"] = lr.predict(test_data_X_sd)\n",
    "LS = test[['PassengerId','Survived']].set_index('PassengerId')\n",
    "\n",
    "#输出结果\n",
    "LS.to_csv('LS5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(svc,param,cv=5,n_jobs=-1,verbose=1,scoring=\"roc_auc\")\n",
    "clf.fit(train_data_X_sd,train_data_Y)\n",
    "\n",
    "clf.best_params_\n",
    "\n",
    "svc = svm.SVC(C=1,max_iter=250)\n",
    "\n",
    "#训练模型并预测结果\n",
    "svc.fit(train_data_X_sd,train_data_Y)\n",
    "svc.predict(test_data_X_sd)\n",
    "\n",
    "#打印结果\n",
    "test[\"Survived\"] = svc.predict(test_data_X_sd)\n",
    "SVM = test[['PassengerId','Survived']].set_index('PassengerId')\n",
    "SVM.to_csv('svm1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=150,min_samples_leaf=3,max_depth=6)\n",
    "\n",
    "xgb_model.fit(train_data_X,train_data_Y)\n",
    "\n",
    "test[\"Survived\"] = xgb_model.predict(test_data_X)\n",
    "XGB = test[['PassengerId','Survived']].set_index('PassengerId')\n",
    "XGB.to_csv('XGB5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbdt = GradientBoostingClassifier(learning_rate=0.7,max_depth=6,n_estimators=100,min_samples_leaf=2)\n",
    "\n",
    "gbdt.fit(train_data_X,train_data_Y)\n",
    "\n",
    "test[\"Survived\"] = gbdt.predict(test_data_X)\n",
    "test[['PassengerId','Survived']].set_index('PassengerId').to_csv('gbdt3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=0.1,max_iter=100)\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(max_depth=6,min_samples_leaf=2,n_estimators=100,num_round = 5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=200,min_samples_leaf=2,max_depth=6,oob_score=True)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf=2,max_depth=6,n_estimators=100)\n",
    "\n",
    "vot = VotingClassifier(estimators=[('lr', lr), ('rf', rf),('gbdt',gbdt),('xgb',xgb_model)], voting='hard')\n",
    "\n",
    "vot.fit(train_data_X_sd,train_data_Y)\n",
    "\n",
    "vot.predict(test_data_X_sd)\n",
    "\n",
    "test[\"Survived\"] = vot.predict(test_data_X_sd)\n",
    "test[['PassengerId','Survived']].set_index('PassengerId').to_csv('vot5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#划分train数据集,调用代码,把数据集名字转成和代码一样\n",
    "X = train_data_X_sd\n",
    "X_predict = test_data_X_sd\n",
    "y = train_data_Y\n",
    "\n",
    "'''模型融合中使用到的各个单模型'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clfs = [LogisticRegression(C=0.1,max_iter=100),\n",
    "        xgb.XGBClassifier(max_depth=6,n_estimators=100,num_round = 5),\n",
    "        RandomForestClassifier(n_estimators=100,max_depth=6,oob_score=True),\n",
    "        GradientBoostingClassifier(learning_rate=0.3,max_depth=6,n_estimators=100)]\n",
    "\n",
    "#创建n_folds\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "#创建零矩阵\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "#建立模型\n",
    "for j, clf in enumerate(clfs):\n",
    "    '''依次训练各个单模型'''\n",
    "    # print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        # print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "# 用建立第二层模型\n",
    "clf2 = LogisticRegression(C=0.1,max_iter=100)\n",
    "clf2.fit(dataset_blend_train, y)\n",
    "y_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test[\"Survived\"] = clf2.predict(dataset_blend_test)\n",
    "test[['PassengerId','Survived']].set_index('PassengerId').to_csv('stack3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
